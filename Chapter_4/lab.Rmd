# Chapter 4 - Lab - Logistic Regression, LDA, QDA, and KNN

## 4.6.1 - Stock Market Data

We examine some numerical and graphical summaries of the `Smarket` data. We first   

```{r lab_4.6.1_setup, message = FALSE}
library(tidyverse)
library(broom)
library(ISLR)
```

```{r lab_4.6.1_1}
(smarket <- as_tibble(Smarket))
 ```

We take a look at the pairwise correlations between the predictors in the set, removing `Direction` because it is quantative.
```{r lab_4.6.1_2}
smarket %>% select(-Direction) %>% cor() %>% tidy()
```

The correlations are all close to zero, with the only larger correlation being between `Year` and `Volume`, as the amount of trades have increased over time:
```{r lab_4.6.1_3}
smarket %>% group_by(Year) %>% summarise(sum(Volume))
```

## 4.6.2 - Logistic Regression

We now use a logistic regression model in order to predict `Direction` using `Lag1 .. Lag5` and `Volume`. 

We recall that trying to use a straight line to fit a binary response that is coded 0|1, there are always p(X) < 0 for some values of X. To avoid this, we must model p(X) with a functon that gives us outputs between 0 and 1. The logistic regression uses the [logistic function](https://en.wikipedia.org/wiki/Logistic_function).

We use the `glm()`, or *generalised linear models* function to achieve this.

Le

```{r lab_4.6.2_1}
smarket.glm <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, smarket, family = binomial)
smarket.glm %>% tidy() %>% arrange(p.value)
```

Looking at the p-values for the predictors, the smallest one is `Lag1` with 0.15, which is larger than our general 0.05 consideration for statistical significance. The negative correlation tells us that if the stock market went up yesterday, it's more likely to go down today.

The `predict()` function can be used to predict the probability that the stocket market will go up given the values of the predictors. The `type = response` option tells R to output probabilities of the form `P(Y = 1|X)` - the probability that Y equals 1 given X. If no data is given to `predict()`, it computes the probabilities for the training data.

Lets add these probabilities as a column `glm.pred` to the `smarket` data.

```{r lab_4.6.2_2}
(smarket <- smarket %>% add_column(glm.pred = predict(smarket.glm, type = "response")))
```

We add another column with the probabilities converted into class labels `Up` and `Down`.

```{r lab_4.6.2_3}
(smarket <- smarket %>% mutate(Pred = ifelse(glm.pred < .5, "Down", "Up")))
```

We'll now create a 'confusion' matrix. In base R this is done using `table()`. but we'll use dplyr functions. We also compute the fraction of days for which the prediction was correct.
```{r lab_4.6.2_4}
smarket %>% group_by(Direction, Pred) %>% tally()
smarket %>% summarise(mean(Direction == Pred))
```

This 0.522, or 52%, means the training error rate is 48%. Training error rates tend to underestimate the test error rate. In order to better assess the model, we'll *hold out* some of the data. We'll train our model on the years 2001 - 2004, and test it against 2005.

```{r lab_4.6.2_5}
smarket_training <- smarket %>% 
    filter(Year < 2005) %>% 
    glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, ., family = binomial)

smarket_test <- smarket %>% 
    filter(Year == 2005) %>% 
    mutate(Prob = predict(smarket_training, ., type = 'response'), Pred = ifelse(Prob < .5, 'Down', 'Up'))

smarket_test %>% group_by(Direction, Pred) %>% tally()
smarket_test %>% summarise(mean(Direction == Pred))
```

We can see that we're now at a mean of 0.48, or 48%, which means our error rate is 52%. This is worse that just guessing.

Let's reduce the regression to only using `Lag1` and `Lag2`, which had the lowest p-values.

```{r lab_4.6.2_6}
smarket_training <- smarket %>% 
    filter(Year < 2005) %>% 
    glm(Direction ~ Lag1 + Lag2, ., family = binomial)

smarket_test <- smarket %>% 
    filter(Year == 2005) %>% 
    mutate(Prob = predict(smarket_training, ., type = 'response'), Pred = ifelse(Prob < .5, 'Down', 'Up'))

smarket_test %>% group_by(Direction, Pred) %>% tally()
smarket_test %>% summarise(mean(Direction == Pred))
```

Slightly better, with 56% of the daily movements predicted. Let's see what the predictions are for certain values of `Lag1` and `Lag2`:
```{r lab_4.6.2_7}
predict(smarket_training, tibble(Lag1 = c(1.2, 1.5), Lag2 = c(1.1, -0.8)), type = 'response')
```

## 4.6.3 - Linear Discriminant Analysis

Now we perform linear discriminant analysis on the data. Logistic regression involves directly modeling Pr(Y = k|X = x) using the logistic function for the case of two response classes.

In the LDA approach, the conditional distribution of the predictors X is modeled in each of the response classes Y, and then use Bayes' theorem to flip these around into estimates for Pr(Y = X|X=x).

```{r lab_4.6.3_setup, message = FALSE}
library(MASS)
```

There doesn't appear to be any broom tidyers for this, so we'll have to do it in a semi base R way.

```{r lab_4.6.3_1}
(smarket_lda_fit <- smarket %>% filter(Year < 2005) %>% lda(Direction ~ Lag1 + Lag2, .))
plot(smarket_lda_fit)
```

We see the prior probabilities of the groups, and the group means which are the average of each predictor within each class. We also see a plot of the linear dicriminants by computing the function with the coefficients over the training data.

```{r lab_4.6.3_2}
(smarket_test <- smarket_test %>% mutate(lda.pred = predict(smarket_lda_fit, .)$class))
smarket_test %>% group_by(Direction, lda.pred) %>% tally()
smarket_test %>% summarise(mean(Direction != lda.pred))
```


## 4.6.4 - Quadratic Discriminant Analysis

We now apply QDA to the stock market data in the same manner.

```{r lab_4.6.4_1}
(smarket_qda_fit <- smarket %>% filter(Year < 2005) %>% qda(Direction ~ Lag1 + Lag2, .))
(smarket_test <- smarket_test %>% mutate(qda.pred = predict(smarket_qda_fit, .)$class))
smarket_test %>% group_by(Direction, qda.pred) %>% tally()
smarket_test %>% summarise(mean(Direction != qda.pred))
```

We now have an error rate of 40.1%, which is reasonably good considering the nature of the stock market.

## 4.6.5 - K-nearest Neighbours

We now perform KNN analysis with the `knn()` function. It's slightly different than the others in that in that rather than a train/test two step, it forms predictions from a single command.

It requires four inputs:

1. A matrix with the predictors of the training data.
1. A matrix with the predictors of the test data.A
1. A vector containing class labels for the training observations.
1. A value for K, the number of nearest neighbours.

```{r 4.6.5_setup, messages = F}
library(class)
```

```{r 4.6.5_1}
smarket_train <- smarket %>% 
    dplyr::filter(Year < 2005) %>% 
    dplyr::select(Lag1, Lag2)

smarket_test <- smarket %>% 
    dplyr::filter(Year == 2005) %>% 
    dplyr::select(Lag1, Lag2)

smarket_K <- smarket %>% 
    dplyr::filter(Year < 2005) %>% 
    dplyr::select(Direction) %>%
    as_vector()

smarket_knn_pred <- knn(smarket_train, smarket_test, smarket_K, k = 1)
```

## 4.6.6 - Caravan Insurance Data

We apply the KNN approach to the `Caravan` data set. It contains 85 predictors for the 5,822 individuals. The response variable is `Purchase`, which indicates whether or not a given individial purchases a caravan insurance policy. The KNN classifier predicts the class of a given test by identifying observations that are nearest to it. Thus the scale of the data matters.

We can standardise the data so that all the data has a mean of 0 and a standard deviation of 1. We do this using the `scale()` function.

```{r 4.6.6_1}
caravan <- as_tibble(Caravan)
std_caravan <- caravan %>% select(-Purchase) %>% scale() %>% as_tibble()
```

We split the observations into training and test sets.

```{r 4.6.6_2}
std_caravan_test <- std_caravan %>% slice(1:1000)
std_caravan_train <- std_caravan %>% slice(1001:nrow(.))
caravan_test_Y <- caravan %>% slice(1:1000) %>% .[['Purchase']]
caravan_train_Y <- caravan %>% slice(1001:nrow(.)) %>% .[['Purchase']]

caravan_knn_pred <- knn(std_caravan_train, std_caravan_test, caravan_train_Y, k = 1)

mean(caravan_knn_pred != caravan_test_Y)
```

The KNN error rate is just udner 12%. This appears to be good, but since only 6% of the customers purchased insurance, we could get the error rate down to 6% by always predicting `No`.

Perhaps the company would like to sell insurance to only those customers who are likely to purchase it. We don't look at the overall error rate, but the error rate for those who are predicted to buy.


```{r 4.6.6_3}
tibble(test_Y = caravan_test_Y, pred_Y = caravan_knn_pred) %>% 
    group_by(pred_Y, test_Y) %>% 
    tally()
```
In this instance, we have `10/(67+10) = 13%`

Let's change K = 5.

```{r 4.6.6_4}
caravan_knn_pred <- knn(std_caravan_train, std_caravan_test, caravan_train_Y, k = 4)

tibble(test_Y = caravan_test_Y, pred_Y = caravan_knn_pred) %>% 
    group_by(pred_Y, test_Y) %>% 
    tally()
```

This gives us `4/(11+4) = 26%`
