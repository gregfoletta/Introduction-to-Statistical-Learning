# 4 - Classification

In many situations, the response variable in a regression may be qualitative instead of quantitative. These are often referred to as **categorical**.

The process for predicting categorical variables is called **classification**. 

We'll be using the `Default` data set, which has annual incomes and monthly credit card balances, with a categorical variable indicating whether an individual defaulted in that month.

```{r setup, message = F}
library(tidyverse)
library(broom)
library(ISLR)
```

```{r 4_1}
default <- as_tibble(Default)
default %>% ggplot() + 
    geom_point(aes(balance, income, colour = default, shape = default), alpha = .5) + 
    scale_shape_manual(values = c(1,4)) + 
    scale_color_manual(values = c('red', 'blue'))

default %>% ggplot() + 
    geom_boxplot(aes(default, balance, fill = default)) + 
    scale_fill_brewer(palette="Dark2")

default %>% ggplot() + 
    geom_boxplot(aes(default, income, fill = default)) + 
    scale_fill_brewer(palette="Dark2")
```

## 4.2 - Why Not Linear?

* When the categorical variable has more than two values, encoding using {1,2,3,...,v} implies an ordering on the outcomes.
* For a binary response, least squares regression makes sense, but some of the estimates can be outside of the [0,1] interval. This makes them difficult to interpret as probabilities.

## 4.3 -Logistic Regression 

Rather than modelling a response directly, logistic regression models the **probability** that `Y` belongs to a particular category. 

### 4.3.1 - The Logistic Model

We wish to model `p(X) = Pr(Y = 1|X)`. We use a function that gives outputs between 0 and 1 for all values of X. In logistic regression, we use the **logistic function**

```
p(X) = e^β_0+β_1X / (1 + e^β_0+β_1X)
```

```{r 4.3.1_1}
tibble(x = -50:50, y = exp(.1* x) / (1 + exp(.1*x))) %>% 
    ggplot(aes(x,y)) + 
    geom_point()
```

To fit the model, a method called **maximum likelyhood** is used. After some manipulation, `p(X) / (1 - p(x) = e^β_0+β_1X`. The quantity on the left is called the *odds*, and it takeson values between 0 and infinity. By taking the log of each side, we have the **log-odds** or **logit** on the left, and that it is linear in X. In a logistic regression model, increasing `X` by one unit changes the log odds by `beta_1`.

### 4.3.2 - Estimating the Regression Coefficients

Maximum likelyhood seeks estimates for the coefficients such that the predicticed probability `p_hat(x_i)` corresponds as closely as possible to the observed status. 

Let's run the model against the default data:

```{r 4.3.1_2}
glm(default ~ balance, family = binomial, default) %>% tidy()
```

We see the `beta_1` is 0.0055, meaning an increase in balance is associated with an increase in the probability of default. It increases the log-odds of default by 0.0055. The **z-statistic** plays the same role as the **t-statistic** in a linear regression.

### 4.3.3 - Making Predictions

Once the coefficients have been determined, you can calculate the probability of the response given a value of the predictor. For qualitative predictors, the same binary encoding used in a linear regression can be used.

### 4.3.4 - Multiple Logistic Regression

The multiple logistic regression is modelled in a similar way to the linear regression - except the additional coefficients are in the exponent to the Euler constant of the logistic function.

The logistic regression can also be performed for more than one response variable. 

## 4.4 - Linear Discriminant Analysis

In the logistic regression, the conditional probability of the response `Y` is modelled given the predictor(s) `X`. In **linear disriminant analysis** we moel the distribution of the predictors `X` separately in each of the response classes `Y`, the use **Bayes' Theorem** to flip these round to estimates for `Pr(Y = k|X = x)`.

Why?

* When the classes are well separated, the parameter estimates for the logistic regression are unstable. LDA does not suffer from this problem.
* If `n` is small and the dsitribution of predictors `X` is approximately normal, LDA is mmore stable.
* LDA is popular if there are more than two response classes.

### 4.4.1 - Using Bayes' Theorem

* `K` classes
* Let `pi_k` represent the **prior probability** than a randomly chosen observation comes from the `k`th class.
* Let `f_k(X) = Pr(X = x|Y=k)` denote the **density function** of `X` for an observation that comes from the `k`th class.
* Bayes' theorem states that:

```
p_k(X) = Pr(Y = k|X = x) = pi_k * f_k(x) / sum[l = 1 -> K](pi_l * f_l(x)
```

Estimating `pi_k` is easy if we have a random sample - we calculate the fraction of training observations that belong to the `k`th class. Estimating `f_k(X)` is more challenging. If we can estimate this function, we can develop a classifier that approximates the Bayes classifier.

`p_k(X)` is the **posterior probability** that `X = x` is in class `k`. 

### 4.4.2 - LDA with p = 1

We would like an estimate for `f_k(x)` that we can use in the formula to calculate `p_k(x)`. We assume the function is normal or **Gaussian**:
```{r 4.4.2_1}
norm_mean <- 0
std_dev <- 20 
tibble(
    x = -100:100, 
    y = (1 / (sqrt(2) * std_dev)) * exp((-1 / (2 * std_dev^2)) * (x - mean)^2)
) %>% ggplot(aes(x,y)) + geom_point()
```

We further assume that $\sigma^2$ is that same amongst all classes. We can plug the Gaussian `f_k(x)` into the `p_l(x)` to get our formula.

Even if we are certain that `X` is Gaussian, we still have to estimate the means, standard deviations and variance. The LDA classifier estimates:
* **Mean**: by summing the `x_i` for the training observation in class `k` and divding by the number of `k` observations.
  * The average of all the training observations in the `k`th class
* **Variance**: is the TSS of the training observations in `k`th class, then summed for each `k` class, divided by the number of training observations minus the number of `K` classes.
  * The weighted average of the sample variances for each of the `K` classes.A
* **Prior Probability**: number of training observations in the `k`th class divided by the total number of observations.

$$ \sum_{i=1}^k $$

This

