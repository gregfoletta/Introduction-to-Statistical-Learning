# Chapter 5 - Lab - Cross-validation and the Bootstrap

```{r setup, message=F}
library(tidyverse)
library(modelr)
library(ISLR)
```

## Validation Set

Let's first use the validation set approach. We use `resample_partition()` to generate a training set and a test set.

We can perform a linear regression with `lm()` on the training observations. We then run the prediction across the test set.

```{r 5_1}
set.seed(1)
auto <- as_tibble(Auto)
auto_sample <- auto %>% resample_partition(c(test = 0.5, train = 0.5))
auto.lm <- lm(mpg ~ horsepower, data = auto_sample$train)
auto %>% slice(auto_sample$test$idx) %>% mse(auto.lm, .)
```

Let's now run across a number of polynomials and see how the MSE changes:

```{r 5_2}
set.seed(1)
auto.lms <- map(1:10, ~lm(mpg ~ poly(horsepower, .x), auto_sample$train))
map_df(1:10, 
    ~ auto %>% 
       slice(auto_sample$test$idx) %>% 
       mse(auto.lms[[.x]], .)
       mutate(poly = .x)
    ) %>% 
    ggplot(aes(poly,MSE)) + 
        geom_point() + 
        geom_line()
```

## Leave-one-out Cross Validation

The LOOCV estimate can be automatically computed for any generalised linear model using the `glm()` and `cv.glm()` functions. The `glm()` will perform a linear regression if no 'family' argument is passed.

A note: I attempted to put these into a pipeline, but this resulted in an error. When the `cv.glm()` calculates all of the LOOCV, it uses a capture of the formula to re-run the regression across while 'leaving on out'. However the '.x' in the map statement isn't visible from the environment where it's run, and so an error is thrown.

```{r 5_3}
library(boot)
1:5 %>% 
  map(~glm(mpg~poly(horsepower, degree = .x), data = auto)) %>% 
  map(~cv.glm(auto, .x)$delta)
```

We can instead run it in a for loop. We then graph the MSE for each degree of polynomial.
```{r 5_4}
cv_error <- rep(0,10)
for (x in 1:10) { 
    fit <- glm(mpg~poly(horsepower,x), data = auto)
    cv_error[x] <- cv.glm(auto, fit)$delta[1] 
}

tibble(x = 1:10, y = cv_error) %>% 
    ggplot(aes(x,y)) + 
        geom_line() + 
        geom_point()
```

## k-fold Cross Validation

The `cv.glm()` can also be used to implement k-fold CV. We pass a `k` variable to the formula to achieve this.
```{r 5_4}
cv_error <- rep(0,10)
for (x in 1:10) { 
    fit <- glm(mpg~poly(horsepower,x), data = auto)
    cv_error[x] <- cv.glm(auto, fit, K = 10)$delta[1] 
}

tibble(x = 1:10, y = cv_error) %>% 
    ggplot(aes(x,y)) + 
        geom_line() + 
        geom_point()
```

## The Bootstrap

