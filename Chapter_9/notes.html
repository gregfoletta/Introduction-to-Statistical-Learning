<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>

<title>9 - Support Vector Machines</title>

<script type="text/javascript">
window.onload = function() {
  var imgs = document.getElementsByTagName('img'), i, img;
  for (i = 0; i < imgs.length; i++) {
    img = imgs[i];
    // center an image if it is the only element of its parent
    if (img.parentElement.childElementCount === 1)
      img.parentElement.style.textAlign = 'center';
  }
};
</script>



<!-- MathJax scripts -->
<script type="text/javascript" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 13px;
}

body {
  max-width: 800px;
  margin: auto;
  padding: 1em;
  line-height: 20px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 {
   font-size:2.2em;
}

h2 {
   font-size:1.8em;
}

h3 {
   font-size:1.4em;
}

h4 {
   font-size:1.0em;
}

h5 {
   font-size:0.9em;
}

h6 {
   font-size:0.8em;
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre, img {
  max-width: 100%;
}
pre {
  overflow-x: auto;
}
pre code {
   display: block; padding: 0.5em;
}

code {
  font-size: 92%;
  border: 1px solid #ccc;
}

code[class] {
  background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * {
      background: transparent !important;
      color: black !important;
      filter:none !important;
      -ms-filter: none !important;
   }

   body {
      font-size:12pt;
      max-width:100%;
   }

   a, a:visited {
      text-decoration: underline;
   }

   hr {
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote {
      padding-right: 1em;
      page-break-inside: avoid;
   }

   tr, img {
      page-break-inside: avoid;
   }

   img {
      max-width: 100% !important;
   }

   @page :left {
      margin: 15mm 20mm 15mm 10mm;
   }

   @page :right {
      margin: 15mm 10mm 15mm 20mm;
   }

   p, h2, h3 {
      orphans: 3; widows: 3;
   }

   h2, h3 {
      page-break-after: avoid;
   }
}
</style>



</head>

<body>
<h1>9 - Support Vector Machines</h1>

<p>The <em>support vector machine</em> is an approach for <strong>classification</strong> that was developed in the computer science community in the 1990s. The are often considered one of the best &#39;out of the box&#39; classifiers.</p>

<p>It is a generalisation of a <em>maximal margin classifier</em> (MMC), which although elegant and simple, cannot be applied to most data sets as it requires the classes be separated by a linear boundary.</p>

<p>The <em>support vector classifier</em> extends the MMC so it can be applied in a broader range of cases.</p>

<p>The <em>support vector machine</em> is a further extension in order to accomodate non-linear class boundaries. SVMs are intended for binary class distinctions, however there are extensions for cases with more than two classes.</p>

<h2>9.1 - Maximal Margin Classifier</h2>

<h3>9.1.1 - Hyperplanes</h3>

<p>In \(p\) dimensional space, a <em>hyperplane</em> is a flat affine subspace of dimension \(p - 1\). In two dimensions, it&#39;s a plane of one-dimension (a line), or in three dimensions its a plane of two-dimensions (a plane).</p>

<p>In two dimensions, a hyperplane is defined by:</p>

<p>\[ \beta_0 + \beta_1X_1 + \beta_2X_2 = 0 \]</p>

<p>By defined we mean that any \(X = (X_1, X_2)^T\) for which the equation holds is a point on the hyperplane.</p>

<p>This can easily be extended out to \(p\) dimensional space:</p>

<p>\[ \beta_0 + \beta_1X_1 + \ldots +  \beta_pX_p = 0 \]</p>

<p>and</p>

<p>\[ X = (X_1, \ldots, X_p)^T \]</p>

<p>If \(X\) doesn&#39;t satisfy the equation, it will be on either one side or another side (\(X < 0\)) or the other side (\(X > 0\)) of the hyperplane. So the hyperplane is dividing the \(p\) dimensional space into two halves.</p>

<h3>9.1.2 - Classification Using a Separating Hyperplane</h3>

<p>Suppose we have an \(n \times p\) matrix \(\matbf{X}\). The \(n\) training observations fall into one of two classes - \(y_1, \ldots, y_n \in \{-1, 1\}\). We also have a test observation \(x^* = (x^*_1, \ldots, x^*_p)^T\). The goal is to develop a classifier that correctly classifies the test observation bases on its features.</p>

<p>We have previously seen:</p>

<ul>
<li>Linear disciminant analysis</li>
<li>Logistic regression</li>
<li>Decision trees</li>
<li>Bagging</li>
<li>Boosting</li>
</ul>

<p>This approach is based on the concept of a separating hyperplane.</p>

<p>We classify \(x^*\) based on the sign of \(f(x^*) = \beta_0 + \beta_1x^*_1 + \ldots + \beta_px^*_p\). If \(f(x^*)\) is positive then the test is assigned to class 1, if it&#39;s negative it&#39;s assigned to class -1. </p>

<p>The magnitude of \(f(x^*)\) can also be used to have increased confidence about the class assignment.</p>

<h3>9.1.3 - Maximal Margin Classifier</h3>

<p>If the data can be perfectly separated by a hyperplane, there are in fact an infinite number of such hyperplanes. Thus we need a reasonable way to contruct the hyperplane.</p>

<p>The <em>maximal margin hyperplane</em> is the farthest away from the training observations. We compute the perdendicular distance from each training observation to the hyperplane. The maximal margin hyperplane is the separating hyperplane that has the farthest minimum distance to the hyperplane.</p>

<p>The classification on this is then known as a <em>maximal margin classifier</em>. Although often useful, there can be overfitting if \(p\) is large.A</p>

<p>Consider the MMC to be the midline of a slab of space between the two classes. Then consider at least three training observations will be equidistant from this midline. These are known as <em>support vectors</em> as they are vectors in \(p\)-dimensional space and they &ldquo;support&rdquo; the MMC in the sense that if they moved, the hyperplane would move as well.</p>

<p>In fact the hyperplane <strong>only</strong> depends on these vectors - a movement of any of the other vectors would not affect the hyperplane.</p>

<h3>Construction of the MMC</h3>

<p>\[ maximise M_{\beta_0, \ldots, \beta_p} \]</p>

<p>\[ \text{subject to } \sum_{j=1}^p\beta_j^2 = 1 \]</p>

<p>\[ y_i(\beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \beta_px_{ip}) \ge M  \forall  i = 1, \ldots, n \]</p>

<p>The final constraint guarrentees that each observation will be on the correct side of the hyperplane, provided \(M\) is positive.</p>

<p>The final two constraints ensure that each observation is on the correct side of the hyperplane, and at least a distance \(M\) from the hyperplane. Hence \(M\) represents the <em>margin</em>. The first constraint then maximises \(M\).</p>

<h3>9.1.5 - The Non-seperable Case</h3>

<p>The MMC is a natural way to perform classification <em>if a separating hyperplane exists</em>. In many cases it may not exist. However the concept can be extended to develop a hyperplane that <em>almost</em> separates using a <em>soft margin</em>. This is known as a <em>support vector classifier</em>.</p>

<h2>9.2 - Support Vector Classifiers</h2>

<h3>9.2.1 - Overview</h3>

<p>Observations belonging to two classes may not be seperable by a hyperplane, and in fact this might not be desirable as it can be sensitive to single observations.</p>

<p>This we might want a hyperplane that doesn&#39;t perfectly separate the two classes in the interest of:</p>

<ul>
<li>Greater robustness to individual observations.</li>
<li>Better classification of <em>most</em> of the training observations.</li>
</ul>

<p>The support vector classifier (or soft margin classifier) does this.</p>

<h3>9.2.2 - Details</h3>

<p>The support vector classifier classifies a test observation based on the side of the hyperplane it is on. The hyperplane is chosen to correct classify <em>most</em> of the training observations, but may misclassify some of them.</p>

<p>It is the solution to the optimisation problem:</p>

<p>\[ maximise M_{\beta_0, \ldots, \beta_p, \epsilon_0, \ldots, \epsilon_n} \]</p>

<p>\[ \text{subject to } \sum_{j=1}^p\beta_j^2 = 1 \]</p>

<p>\[ y_i(\beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \beta_px_{ip}) \ge M(1 - \epsilon_i) \]</p>

<p>\[ \epsilon_i \ge 0, \sum_{i=1}^n \epsilon_i \le C \]</p>

<p>where \(C\) is a non-negative tuning parameter. As with the MMC \(M\) is the margin we seek to make as large as possible. The \(\epsilon_0, \ldots, \epsilon_n\) are <em>slack variables</em> that allow individual observations to be on the wrong side of the margin.</p>

<p>The slack variable tells us where the observation is located. If \(\epsilon_i = 0\) then the $i$th observation is on the right side of the margin. If it&#39;s \(e_i > 0\) then the $i$th observation is on the wrong side of the margin and has <em>violated</em> the margin. If \(\epsilon_i > 0\) then it&#39;s on the wrong side of the hyperplane.</p>

<p>\(C\) bounds the sum of the \(\epsilon_i\) and so determines the number and severity of the violations to the margin we will tolerate. It can be considered the <em>budget</em> for the amount the margin can be violated by the \(n\) observations.</p>

<p>For \(C > 0\) no more than \(C\) observations can be on the wrong side of the hyperplane, because \(\epsilon_i > 1\) for those observations. \(C\) is treated as a tuning parameter selected by cross-validation.</p>

<p>It has an interesting property that only observations that lie on the margin or that violate the margin affect the hyperplane. The fact that the support vector classifier&#39;s decision rule is based only on a small subset of the training observations means it is robust to the behaviour of observations far away from the hyperplane.</p>

<h2>9.3 - Support Vector Machines</h2>

<h3>9.3.1 - Classification with Non-linear Decision Boundaries</h3>

<p>The support vector classifier is a natural approach in the two class setting if the decision boundary is linear. However in practice we often deal with non-linear decision boundaries.</p>

<p>With linear regression we enlarged the feature space with functions of the predictors in order to address non-linearity. With a support vector classifier we could address the issue in a similar way by enlarging the feature space using quadratic, cubic, &hellip;, functions:</p>

<p>\[ X_1, X_1^2, X_2, X_2^2, \ldots, X_p, X_p^2 \]</p>

<p>In the enlarged feature space, the decision boundary is still linear, however in the original feature space the decision boundary is of the form \(q(x) = 0\) where \(q\) is a quadratic polynomial whose solutions are generally non-linear.</p>

<p>There are a number of other ways to enlarge the feature space, and we could end up in a situation where there are a huge number of features making it computationally infeasible.</p>

<p>The support vector machine allows us to enlarge the fature space used by the support vector classifier in a way that leads to efficient computations.</p>

<h3>9.3.2 - Support Vector Machine</h3>

<p>The <em>support vector machine</em> (SVM) is an extension of the support vector classifier that results in enlarging the feature space in a specific way: using <em>kernels</em>.</p>

<p>It turns out that the solution to the support vector classifier problem involves only the <em>inner products</em> of the observations, not the observations themselves. The inner product for \(r\)-vectors \(a\) and \(b\) is \(\langle a,b \rangle \sum_{i=1}^r a_ib_i\).</p>

<p>It can be shown that:</p>

<ul>
<li>The linear support vector classifier can be represented as:</li>
</ul>

<p>\[ f(x) = \beta_0 + \sum_{i=1}^n \alpha_i \langle x,x_i \rangle \]</p>

</body>

</html>
