# Chapter 3 - Notes

## 3.1 - Simple Linear Regression

A simple linear regression predicts a response `Y` based on a single predictor variable `X`. 
```
Y ≈ β_0 + β_1X
```

Once we have training data to produce estimates for the coefficients, we can predict values by computing:
```
ŷ = β̂ 0 + β̂ 1 x
```

### 3.1.1 - Estimating Coefficients

The goal is to obtain coefficients such that the linear model fits the data well - i.e. as close as possible to the data points. The most common approach involves minimising the **least squares** criterion.

We let `e_i = y_i − ŷ_i`, which represents the *i*th **residual**. The **residual sum of squares** or **RSS** is the sum of the squares of each of these residuals.

An example in R - we generate some data and calculate the residual sum of squares.

```{r 3.1.1 setup, message = F}
library(tidyverse)
```
```{r 3.1.1_1}
set.seed(1)
y_i <- rnorm(10)
y_hat_i <- rnorm(10)
(RSS = sum((y_i - y_hat_i)^2))
```


