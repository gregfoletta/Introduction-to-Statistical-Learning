# Chapter 3 - Applied

## 8) Auto data set

```{r applied_setup, echo = F, message = F}
library(tidyverse)
library(broom)
library(ISLR)
```

### a)
*Use the `lm()` function to perform a simple linear regression with `mpg` as the response and `horsepower` as the predictor. Print the results and comment on the output.*

```{r applied_auto_linear}
auto <- as_tibble(Auto)
lm_auto <- auto %>% lm(mpg ~ horsepower, .)
lm_auto %>% tidy()
```

#### i) 
*Is there a relationship between the predictor and the response?*

To determine if there is a relationship, we need to look at two items: the p-values for the coefficients, the F-statistic, and the p-value for the F-statistic:
```{r applied_p_val_and_f_stat}
lm_auto %>% tidy()
lm_auto %>% glance()

```

We in the `tidy()` output we can see the p-value for the intercept and the coefficient is very small, indicating a very low probability for the null hypothesis.A

In the `glance()` output, we see the `statistic` column (the F-statistic) is high at around 600, with a small p-value. This indicates that there is a relationship.

#### ii)
*How strong is the relationship*

To test how string the relationship is, we can look at the residual standard error (RSE) and the R^2 value.

We look at the `glance()` output again and see the R^2 value is .606. Recall that the R^2 is a value between 0 and 1 that is the 'proportion of the variance explained'. We can consider the relationship reasonably strong;

#### iii)
*Is the relationship positive or negative?*
There is a negative relationship between `mpg` and `horsepower`, thus miles per gallon goes down as horsepower goes up. This aligns with our conceptual idea.

#### iv) 
*What is the predicted `mpg` associated with a `horsepower` of 98? What are the associated 95% confidence and prediction intervals?*

We use the `interval` argument to the predict function. Note that the default `level` argument of `predict()` is 0.95 (95%).
```{r applied_linear_prediction}
predict(lm_auto, tibble(horsepower = 98), interval = 'confidence')
predict(lm_auto, tibble(horsepower = 98), interval = 'predict')
```

### b)
*Plot the response and the predictor. Display the least squares regression line.*

```{r applied_auto_mpg_hp}
auto %>% ggplot(aes(mpg, horsepower)) + geom_point() + geom_smooth(method = 'lm', formula = 'y ~ x')
```

### c) 
*Produce diagnostic plots (Resid v Leverage, Resid v Fitted, Fitted v Std Resid) and comment on any problems*

First off, lets have a look at the residuals versus the leverage:
```{r applied_auto_leverage_v_fitted}
augment(lm_auto) %>% ggplot(aes(.hat, .resid)) + geom_point()
```

There are a few points up in the top right. We take a look at the Cook's distance for the observations.

```{r applied_auto_cooks_distance}
augment(lm_auto) %>% mutate(i = 1:n()) %>% ggplot(aes(i, .cooksd)) + geom_bar(stat = 'identity')
```

A couple of high points but all below 1.

Now we look at the fitted versus the residuals, and also fit a quadratic regression. We see a bit of a U shape, indicating potential non-linearity in the data.
```{r applied_auto_fitted_v_residuals}
augment(lm_auto) %>% ggplot(aes(.fitted, .resid)) + geom_point() + geom_smooth(method = 'lm', formula = 'y ~ poly(x,2)')
```

## 9) Multiple Linear Regression - Auto Data Set

```{r applied_mult_auto_setup, message = F}
library(GGally)
library(corrplot)
```

### a)
*Produce a scatterplot matrix which includes all the data in the data set*
```{r applied_mult_auto_pairs, message = F}
auto %>% select(-'name') %>% ggpairs()
```

### b)
*Compute the matrix of correlations between the variables.*
```{r applied_mult_auto_corr}
auto %>% select(-'name') %>% cor() %>% corrplot(method = 'color')
```

### c)
*Perform a multiple linear regression with `mpg` as the response and all other variables except `name` as the predictors.*

```{r applied_mult_auto_lm}
lin_reg_auto <- lm(mpg ~ . -name, auto)
tidy(lin_reg_auto)
glance(lin_reg_auto)
```
#### i)
*Is there a relationship between the predictors and the response?*
We test the null hypothesis of "are all of the regression coefficients zero?". The F-statistic 252 (far greater than 1) and has a p-value of 2e-139, indicating a low probability that this is just by chance. We can therefore say there is a relationship between the predictors and the response.

#### ii)
We look at the p-values for each of the predictors. The predictors which have a high probability of having an effect on the `mpg`, holding all others constant, appear to be `weight`, `year`, `displacement` and `origin`.

#### iii)
*What does the coefficient for the `year` variable suggest?*
The year coefficient suggests that a cars `mpg` gets larger - and therefore better - the later a car was made.

### d)
*Plot the diagnostic plots of the regression and comment on any problems with the fit.*

We go through our usual plots - first off is looking at the residuals versus the leverage:
```{r applied_multi_auto_leverage}
augment(lin_reg_auto) %>% mutate(i = 1:n()) %>% ggplot(aes(i, .cooksd)) + geom_bar(stat = 'identity')
```

We don't see values with a significant Cook's distance. We move on to the fitted versus the residuals:
```{r applied_auto_fitted_resid}
augment(lin_reg_auto) %>% ggplot(aes(.fitted, .resid)) + geom_point() + geom_smooth(method = 'lm', formula = 'y~poly(x,2)')
```
There is some evidence of the non-linearity of the results.

### e) 
*Use the `*` and `:` symbols to fit linear regressions with interaction effects. Are any interactions statistically significant?*
A `*` adds the predictors and the interaction term, whereas the `:` only adds the interaction term. I.e. `x\*y == x + y + x:y`.

Let's have a think about potential interactions - I think weight and year could interact, given the changes in materials. There could also be an f,Let's have a think about potential interactions - I think weight and year could interact, given the changes in materials. There could also be and interaction between cylinders and displacement:

```{r applied_auto_interaction}
lm(mpg ~ weight*year + cylinders*displacement, auto) %>% tidy()
```

All of the values appear to be reasonably statistically significant. In fact, if we have a look at the fitted vs residuals, it looks much better than before:
```{r applied_auto_interaction_fitted}
lm(mpg ~ weight*year + cylinders*displacement, auto) %>% augment() %>% ggplot(aes(.fitted, .resid)) + geom_point() + geom_smooth()A
```

### f)
*Try different transformations of the variables and comment ont the findings.*

We try a few different transformations and pipe them through to the fitted versus residuals graph:

```{r applied_auto_transformations}
lm(mpg ~ sqrt(horsepower), auto) %>% augment() %>% ggplot() + geom_point(aes(.fitted, .resid))
lm(mpg ~ log(horsepower), auto) %>% augment() %>% ggplot() + geom_point(aes(.fitted, .resid))
lm(1/mpg ~ horsepower, auto) %>% augment() %>% ggplot() + geom_point(aes(.fitted, .resid))
lm(1/mpg ~ horsepower + weight*year, auto) %>% augment() %>% ggplot() + geom_point(aes(.fitted, .resid))
lm(1/mpg ~ horsepower + weight*year, auto) %>% glance()
```

The last one looks quite good.


## 10) Carseats Data Set

### a) 
*Fit a multiple regression model to predict `Sales` using `Price`, `Urban`, and `US`.*

```{r applied_carseats_10_a}
carseats <- as_tibble(Carseats)
cs_regress <- lm(Sales ~ Price + Urban + US, carseats)
cs_regress %>% tidy()
```






