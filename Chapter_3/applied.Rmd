# Chapter 3 - Applied

## 8) Auto data set

```{r applied_setup, echo = F, message = F}
library(tidyverse)
library(broom)
library(ISLR)
```

### a)
*Use the `lm()` function to perform a simple linear regression with `mpg` as the response and `horsepower` as the predictor. Print the results and comment on the output.*

```{r applied_auto_linear}
auto <- as_tibble(Auto)
lm_auto <- auto %>% lm(mpg ~ horsepower, .)
lm_auto %>% tidy()
```

#### i) 
*Is there a relationship between the predictor and the response?*

To determine if there is a relationship, we need to look at two items: the p-values for the coefficients, the F-statistic, and the p-value for the F-statistic:
```{r applied_p_val_and_f_stat}
lm_auto %>% tidy()
lm_auto %>% glance()

```

We in the `tidy()` output we can see the p-value for the intercept and the coefficient is very small, indicating a very low probability for the null hypothesis.A

In the `glance()` output, we see the `statistic` column (the F-statistic) is high at around 600, with a small p-value. This indicates that there is a relationship.

#### ii)
*How strong is the relationship*

To test how string the relationship is, we can look at the residual standard error (RSE) and the R^2 value.

We look at the `glance()` output again and see the R^2 value is .606. Recall that the R^2 is a value between 0 and 1 that is the 'proportion of the variance explained'. We can consider the relationship reasonably strong;

#### iii)
*Is the relationship positive or negative?*
There is a negative relationship between `mpg` and `horsepower`, thus miles per gallon goes down as horsepower goes up. This aligns with our conceptual idea.

#### iv) 
*What is the predicted `mpg` associated with a `horsepower` of 98? What are the associated 95% confidence and prediction intervals?*

We use the `interval` argument to the predict function. Note that the default `level` argument of `predict()` is 0.95 (95%).
```{r applied_linear_prediction}
predict(lm_auto, tibble(horsepower = 98), interval = 'confidence')
predict(lm_auto, tibble(horsepower = 98), interval = 'predict')
```

### b)
*Plot the response and the predictor. Display the least squares regression line.*

```{r applied_auto_mpg_hp}
auto %>% ggplot(aes(mpg, horsepower)) + geom_point() + geom_smooth(method = 'lm', formula = 'y ~ x')
```

### c) 
*Produce diagnostic plots (Resid v Leverage, Resid v Fitted, Fitted v Std Resid) and comment on any problems*

First off, lets have a look at the residuals versus the leverage:
```{r applied_auto_leverage_v_fitted}
augment(lm_auto) %>% ggplot(aes(.hat, .resid)) + geom_point()
```

There are a few points up in the top right. We take a look at the Cook's distance for the observations.

```{r applied_auto_cooks_distance}
augment(lm_auto) %>% mutate(i = 1:n()) %>% ggplot(aes(i, .cooksd)) + geom_bar(stat = 'identity')
```

A couple of high points but all below 1.

Now we look at the fitted versus the residuals, and also fit a quadratic regression. We see a bit of a U shape, indicating potential non-linearity in the data.
```{r applied_auto_fitted_v_residuals}
augment(lm_auto) %>% ggplot(aes(.fitted, .resid)) + geom_point() + geom_smooth(method = 'lm', formula = 'y ~ poly(x,2)')
```

## 9) Multiple Linear Regression - Auto Data Set

```{r applied_mult_auto_setup, message = F}
library(GGally)
library(corrplot)
```

### a)
*Produce a scatterplot matrix which includes all the data in the data set*
```{r applied_mult_auto_pairs, message = F}
auto %>% select(-'name') %>% ggpairs()
```

### b)
*Compute the matrix of correlations between the variables.*
```{r applied_mult_auto_corr}
auto %>% select(-'name') %>% cor() %>% corrplot(method = 'color')
```

### c)
*Perform a multiple linear regression with `mpg` as the response and all other variables except `name` as the predictors.*

```{r applied_mult_auto_lm}
lin_reg_auto <- lm(mpg ~ . -name, auto)
tidy(lin_reg_auto)
glance(lin_reg_auto)
```
#### i)
*Is there a relationship between the predictors and the response?*
We test the null hypothesis of "are all of the regression coefficients zero?". The F-statistic 252 (far greater than 1) and has a p-value of 2e-139, indicating a low probability that this is just by chance. We can therefore say there is a relationship between the predictors and the response.

#### ii)
We look at the p-values for each of the predictors. The predictors which have a high probability of having an effect on the `mpg`, holding all others constant, appear to be `weight`, `year`, `displacement` and `origin`.

#### iii)
*What does the coefficient for the `year` variable suggest?*
The year coefficient suggests that a cars `mpg` gets larger - and therefore better - the later a car was made.

### d)
*Plot the diagnostic plots of the regression and comment on any problems with the fit.*

We go through our usual plots - first off is looking at the residuals versus the leverage:
```{r applied_multi_auto_leverage}
augment(lin_reg_auto) %>% mutate(i = 1:n()) %>% ggplot(aes(i, .cooksd)) + geom_bar(stat = 'identity')
```

We don't see values with a significant Cook's distance. We move on to the fitted versus the residuals:
```{r applied_auto_fitted_resid}
augment(lin_reg_auto) %>% ggplot(aes(.fitted, .resid)) + geom_point() + geom_smooth(method = 'lm', formula = 'y~poly(x,2)')
```
There is some evidence of the non-linearity of the results.

### e) 
*Use the `*` and `:` symbols to fit linear regressions with interaction effects. Are any interactions statistically significant?*
A `*` adds the predictors and the interaction term, whereas the `:` only adds the interaction term. I.e. `x\*y == x + y + x:y`.

Let's have a think about potential interactions - I think weight and year could interact, given the changes in materials. There could also be an f,Let's have a think about potential interactions - I think weight and year could interact, given the changes in materials. There could also be and interaction between cylinders and displacement:

```{r applied_auto_interaction}
lm(mpg ~ weight*year + cylinders*displacement, auto) %>% tidy()
```

All of the values appear to be reasonably statistically significant. In fact, if we have a look at the fitted vs residuals, it looks much better than before:
```{r applied_auto_interaction_fitted}
lm(mpg ~ weight*year + cylinders*displacement, auto) %>% augment() %>% ggplot(aes(.fitted, .resid)) + geom_point() + geom_smooth()
```

### f)
*Try different transformations of the variables and comment ont the findings.*

We try a few different transformations and pipe them through to the fitted versus residuals graph:

```{r applied_auto_transformations}
lm(mpg ~ sqrt(horsepower), auto) %>% augment() %>% ggplot() + geom_point(aes(.fitted, .resid))
lm(mpg ~ log(horsepower), auto) %>% augment() %>% ggplot() + geom_point(aes(.fitted, .resid))
lm(1/mpg ~ horsepower, auto) %>% augment() %>% ggplot() + geom_point(aes(.fitted, .resid))
lm(1/mpg ~ horsepower + weight*year, auto) %>% augment() %>% ggplot() + geom_point(aes(.fitted, .resid))
lm(1/mpg ~ horsepower + weight*year, auto) %>% glance()
```

The last one looks quite good.


## 10) Carseats Data Set

### a) 
*Fit a multiple regression model to predict `Sales` using `Price`, `Urban`, and `US`.*

```{r applied_carseats_10_a}
carseats <- as_tibble(Carseats)
cs_regress <- lm(Sales ~ Price + Urban + US, carseats)
cs_regress %>% tidy()
```

### b)
*Provide an interpretation of each coefficient in the model.*
* (Intercept) - the average number of sales of carseats, ignoring all other factors. 
* Price - the regression indicates a relationship between price and sales, given the low p-value of the t-statistic. An increase in price of a dollar results in a decrease of 54 carseats solds. 
* UrbanYes - given the high p-value, there doesn't appear to be a relationship between sales and whether a store is urban.
* USYes - given the low p-value, the store bein in the US results in 1200 more carseats being sold.

### c)
*Write out the model in equation form, being careful to handle the qualitative variables properly.*

`Sales = x * Price + y * Urban + z * US`, where 
`[Urban = Yes => y = 1|Urban = No =>  y = 0]` & 
`[US = Yes => z = 1|US = No => z = 0]`

### d)
*For which of the predictors can you reject the null hypothesis H 0 : βj = 0?*

The null hypothesis can be rejected for `Price` and `US`.

### e)
*On the basis of your response to the previous question, fit a smaller model that only uses the predictors for which there is evidence of association with the outcome.*

```{r applied_carseats_10_e}
cs_regress_reduced <- lm(Sales ~ Price + US, carseats)
cs_regress_reduced %>% tidy()
```

### f)
*How well do the models in (a) and (e) fit the data?*

We look at the F-statistic and its assoicated p-value to determine how well the models fit:
```{r applied_carseats_10_f}
cs_regress %>% glance()
cs_regress_reduced %>% glance()
```

We see no increase in the R-sqaured value - but we do see an increase in the F-statistic, and a decrease in its p-value.

### g)
*Using the model from (e), obtain 95 % confidence intervals for the coefficient(s).*

```{r applied_carseats_10_g}
cs_regress_reduced %>% confint()
```

### h)
*Is there evidence of outliers or high leverage observations in the model from (e)?*

```{r applied_carseats_10_h}
cs_regress_reduced %>% augment() %>% ggplot(aes(.fitted, .resid)) + geom_point()
cs_regress_reduced %>% augment() %>% ggplot(aes(.hat, .resid)) + geom_point()
```

There doesn't appear to be any non-linearity in the daata, and the high leverage points don't appear to affect the data substantially.

## 11) T-statistic and null-hypothesis
*In this problem we will investigate the t-statistic for the null hypothesis H 0 : β = 0 in simple linear regression without an intercept. To begin, we generate a predictor x and a response y*

```{r applied_11}
set.seed(1)
linear_response <- tibble(x = rnorm(100), y = 2 * x + rnorm(100))
linear_response %>% ggplot(aes(x,y)) + geom_point()
```

### a) 
*Perform a simple linear regression of y onto x , without an intercept. Report the coefficient estimate β̂, the standard error of this coefficient estimate, and the t-statistic and p-value associated with the null hypothesis H 0 : β = 0. Comment on these results.*

```{r applied_11_a}
lr_reg <- lm(y ~ x + 0, linear_response)
lr_reg %>% tidy()
```

The coefficient estimate is 1.99 - very close to the 2 that we used to generate the points. The standard error is .106, the t-statistic is 18.7 and the p-value is 2.64e-34. The t-statistic tells us how many standard deviations the coefficient is away from 0. The p-value gives us the probability that the null hypothesis - that the coefficient is 0 - is true.

### b) 
*Now perform a simple linear regression of x onto y without an intercept, and report on the same metrics as a)*

```{r applied_11_b}
lr_reg_reverse <- lm(x ~ y + 0, linear_response)
lr_reg_reverse %>% tidy()
```

The coefficient estimate and the standard error have changed, which is as expected. The t-statistic and p-value remain the same.

### c)
*What is the relationship between the results obtained in (a) and (b)?*

It's the same line, thus the overall results obtained are the same.

## 12) Simple Linear Regression

### a) 
*Recall that the coefficient estimate β̂ for the linear regression of Y onto X without an intercept is given by (3.38). Under what circumstance is the coefficient estimate for the regression of X onto Y the same as the coefficient estimate for the regression of Y onto X?*

When the sum of the squares of the y observations is the same as the sum or the squares of the x observations.

### b)
Generate an example in R with n = 100 observations in which the coefficient estimate for the regression of X onto Y is different from the coefficient estimate for the regression of Y onto X.

```{r applied_12_b}
set.seed(1)
tibble(x = rnorm(100), y = 4*x) %>% lm(y ~ x, .) %>% tidy()
set.seed(1)
tibble(x = rnorm(100), y = 4*x) %>% lm(x ~ y, .) %>% tidy()
```

### c)
*Generate an example in R with n = 100 observations in which the coefficient estimate for the regression of X onto Y is the Asame as the coefficient estimate for the regression of Y onto X.*

To have the same coefficients, we need the sum of the squares to be the same. To do this, we generate 100 random values for `X`, and we use the same values for `Y`. However we re-order the values so we don't simply get a `y = x` function. We can use the `sample_n()` function for this:

```{r applied_12_c}
set.seed(1)
sample_data <- tibble(x = rnorm(100)) %>% 
    mutate(y = (sample_n(., 100) %>% .[['x']]))
lm(y ~ x, sample_data) %>% tidy()
lm(x ~ y, sample_data) %>% tidy()
```

## 13) Simulated Linear Regressions

### a)
*Using the rnorm() function, create a vector, x , containing 100 observations drawn from a N (0, 1) distribution. This represents a feature, X.*

```{r applied_13_a}
x <- rnorm(100)
```

### b) 
*Using the `rnorm()` function, create a vector, `eps` , containing 100 observations drawn from a N (0, 0.25) distribution*

```{r applied_13_b}
eps <- rnorm(100, 0, .25)
```

### c) 
*Using `x` and `eps` , generate a vector y according to the model `Y = −1 + 0.5X + e`. What is the length of the vector y ? What are the values of β 0 and β 1 in this linear model?*

```{r applied_13_c}
y <- -1 + .5 * x + eps
```

The beta_0 is -1, and the beta_1 is .5

### d)
*Create a scatterplot displaying the relationship between x and y . Comment on what you observe.

```{r applied_13_d}
simulated <- tibble(x = x, y = y) 
simulated %>% ggplot(aes(x,y)) + geom_point()
```

We can see an approximate linear relationship between x and y.

### e)
*Fit a least squares linear model to predict y using x . Comment on the model obtained. How do β̂ 0 and β̂ 1 compare to β 0 and β 1 ?*

```{r appled_13_e}
lm(y ~ x, simulated) %>% tidy()
```

We see a `beta_0` of -0.988, and a `beta_1` of 0.527 - very close to the actual values of the function. The standard error is low at 0.0248 and 0.0241 respectively.

### f)
*Display the least squares line on the scatterplot obtained in (d). Draw the population regression line on the plot, in a different color. Create a legend.

```{r applied_13_f}
simulated %>% ggplot(aes(x,y)) + 
    geom_point() + 
    geom_smooth(aes(colour = 'Regression'), method = 'lm', formula = 'y ~ x') + 
    geom_abline(aes(slope = .5, intercept = -1, colour = 'Population Mean'), size = 1) +
    labs(colour = "Lines")
```

### g) 
*Now fit a polynomial regression model that predicts `y` using `x` and `x^2` . Is there evidence that the quadratic term improves the model fit? Explain your answer.*

```{r applied_13_g}
lm(y ~ poly(x,2), simulated) %>% tidy()
lm(y ~ poly(x,2), simulated) %>% glance()
```

We can see that the R^2 has not changed, and the RSE has increased slightly with the x^2 regression. However the F-statistic has decreased significantly with the x^2 regression, indicating a decrease in the significance of the model.

### h) & i)
*Repeat (a)–(f) after modifying the data generation process in such a way that there is less noise in the data.*

Lets create a function to do this, with the variable being how much noise is in the data.

```{r applied_13_h_1}
simulated_linear <- function(observations, mean, noise) {
    x <- rnorm(observations)
    eps <- rnorm(observations, mean, noise)
    y <- -1 + .5 * x + eps
    return(tibble(x = x, y = y))
}

set.seed(1)
low_and_high_noise <- bind_cols( simulated_linear(100, 0, .1), simulated_linear(100, 0, .5))
lm(y ~ x, low_and_high_noise) %>% glance()
lm(y ~ x, low_and_high_noise) %>% tidy()
lm(y1 ~ x1, low_and_high_noise) %>% tidy()
lm(y1 ~ x1, low_and_high_noise) %>% glance()
```

Looking at the values, the low noise regression picks the exact coefficients that were used in the function. The R^2 is near 1 with a high F-statistic.

The higher noise, as expected, has low F-statistic and high p-values for the coefficients. The R^2 value is very low.

```{r applied_13_h_2}
low_and_high_noise %>% 
    ggplot() + 
    geom_point(aes(x, y), colour = 'red') + 
    geom_point(aes(x1, y1), colour = 'blue') + 
    geom_smooth(aes(x, y), method = 'lm', formula = 'y ~ x', colour = 'red') + 
    geom_smooth(aes(x1, y1), method = 'lm', formula = 'y ~ x', colour = 'blue')
```

### j)
* What are the confidence intervals of the data sets? Comment on the results*

```{r applied_13_j}
lm(y ~ x, simulated) %>% confint()
lm(y ~ x, low_and_high_noise) %>% confint()
lm(y1 ~ x1, low_and_high_noise) %>% confint()
```

## 14) Collinearity Problem

### a) 
*Set up the data*
```{r applied_14_a}
set.seed(1)
colin_data <- tibble(
    x1 = runif(100), 
    x2 = 0.5 * x1 + rnorm(100)/10, 
    y = 2 + 2*x1 + 0.3*x2 + rnorm(100)
)
```

The beta_{0,1,2} coefficients of the model are (2,2,0.3) respectively.

### b) 
*What is the correlation between x1 and x2 ? Create a scatterplot displaying the relationship between the variables.*

```{r applied_14_b}
colin_data %>% select(x1, x2) %>% cor()

colin_data %>% ggplot(aes(x1, x2)) + geom_point()
```

### c) 
*Using this data, fit a least squares regression to predict y using `x1` and `x2`*

```{r applied_14_c}
colin_data_reg <- lm(y ~ x1 + x2, colin_data)
colin_data_reg %>% tidy()
colin_data_reg %>% glance()
```

*Can the null hypotheses `beta_1 = 0` and `beta_2 = 0` be rejected?*

Let's take the general view that a p-value of 0.05 is statistically significant. With this in mind, we can reject the null hypothesis for `x1`, but not for `x2`.


### d) and e)
*Now fit a least squares regression to predict y using only x1, with only x2. Can we reject the null hypothesis for either of these?*

```{r applied_14_d_e}
colin_data_reg_x1 <- lm(y ~ x1, colin_data)
colin_data_reg_x2 <- lm(y ~ x2, colin_data)
colin_data_reg_x1 %>% tidy()
colin_data_reg_x2 %>% tidy()
```

I both instances we have a p-value below 0.05, and thus we can rejct the null hypothesis in both instances.

### f) 
*Do the results obtained in (c)–(e) contradict each other? Explain your answer.*

The results don't contradict each other. When regressing using both predictors that a colinear, it can be difficult to separate out the individual effects on the response, and the power of the hypothesi stest is reduced. (see ISL 3.36).

### g) 
*Now suppose we obtain one additional observation, which was unfortunately mismeasured. Re-fit the linear models from (c) to (e) using this new data. What effect does this new observation have on the each of the models? In each model, is this observation an outlier? A high-leverage point? Both? Explain your answers*

We add the new observation and calculate the mew regressions.

```{r applied_14_g_a_1}
colin_data_add <- colin_data %>% add_row(x1 = 0.1, x2 = 0.8, y = 6)

colin_data_reg_both <- lm(y ~ x1 + x2, colin_data_add)
colin_data_reg_x1 <- lm(y ~ x1, colin_data_add)
colin_data_reg_x2 <- lm(y ~ x2, colin_data_add)
```

We recall from 3.4 that an outlier is a point far from the value predicted by the model. We can look at residuals, but it can be difficult to decide how large a residual needs to be before it's classified as an outlier. This is where *studentised residuals* are used, where each residual is divided by e_i - its estimated standard error. Points with standardised residuals greater than 3 are possible outliers.

```{r_applied_14_g_a_2}
colin_data_reg_both %>% augment() %>% filter(y == 6)
colin_data_reg_x1 %>% augment() %>% filter(y == 6)
colin_data_reg_x2 %>% augment() %>% filter(y == 6)
```
Only in the regression with `x1` as the predictor could the response be considered an outlier.

Points with high leverage have an unusual x_i value, which tend to have significant impacts on the estimated regression lines. The *leverage statistic* is used to quantify leverage. We look at the `.hat` column which gives us the leverage statistic. We plot the `.hat` versus the `.std.resid`. When both `x1`, and `x2` are used in the regression, this point appears to be a high leverage point. In the `x1` regression it's not a high leverage point, and in the `x2` regression it has a bit of leverage.

```{r_applied_14_g_a_3}
colin_data_reg_both %>% augment() %>% ggplot() + geom_point(aes(.hat, .std.resid))
colin_data_reg_x1 %>% augment() %>% ggplot() + geom_point(aes(.hat, .std.resid))
colin_data_reg_x2 %>% augment() %>% ggplot() + geom_point(aes(.hat, .std.resid))
```

## 15) Boston Data Set
*We will now try to predict per capita crime rate using the other variables in this data set.*

### a)
*For each predictor, fit a simple linear regression model to predict the response. Describe your results. In which of the models is there a statistically significant association between the predictor and the response? Create some plots to back up your assertions.*

```{r applied_15_a}
library(MASS)
boston <- as_tibble(Boston)

tibble(
    predictor = names(boston)[-1]) %>% mutate(predictor %>% map(function(x) lm(paste('crim ~', x), boston) %>% tidy())
    ) %>% unnest() %>% print(n = 26)
```


