---
title: "Chapter 10 - Unsupervised Learning - Lab Exercises"
author: "Greg Foletta"
output:
  html_document:
    theme: cerulean
    highlight: tango
---

```{r private_setup, include = F}
library(knitr)
library(kableExtra)
knitr::opts_chunk$set(dpi = 300, dev.args = list(type = "cairo"))
```

```{r setup, message=F}
library(broom)
library(modelr)
library(tidyverse)
library(modelr)
library(ISLR)
library(ggdendro)
library(ggfortify)
```

## 10.4 - Principal Component Analysis

We will perform PCA on the `USArrests` data set. It contains the 50 states in alphabetical order, with the urban population, and the number of assaults, rapes and murders.

```{r 1}
(us_arrests <- as_tibble(USArrests, rownames = 'State'))
```

We take a look at the data, specifically the means and the variance for each variable:

```{r 2}
us_arrests %>%
    gather(key = 'Key', value = 'Value', c(Murder, Assault, Rape, UrbanPop)) %>%
    group_by(Key) %>%
    summarise(average = mean(Value), variance = var(Value))
```

The variables have vastly different means and variances, meaning we will need to scale before PCA is performed. If we didn't, most of the principal components would be driver by the `Assault` variable since it has the largest mean and variance.

By default `prcomp()` centers the variables with mean zero. The `scale` argument scales the standard deviations to one.

```{r 3}
arrests_pca <- us_arrests %>%
    select(-State) %>%
    prcomp(scale = T)

arrests_pca
```

The rotation matrix provides the principal component loadings - each column is the loading vector for each principal component. It's called the rotation matrix because when we multiply it with $X$ we get the coordinates of the data in the rotated coordinate system.

```{r 4}
arrests_pca$rotation
```

We don't need to multiply these matrices ourselves - the rotated matrix is in `x`:

```{r 5}
arrests_pca$x[1:4,]
```

We plot the first two principal components:
```{r 6}
arrests_pca %>% biplot(scale = 0)
```

The `prcomp()` function also outputs the standard deviation. From this we can calculate the proportion of variance explained by each principal component.

```{r 7}
arrests_pca$sdev^2 / sum(arrests_pca$sdev^2)

tibble(
    pc = c('PC1', 'PC2', 'PC3', 'PC4'),
    pve = arrests_pca$sdev^2 / sum(arrests_pca$sdev^2)
) %>%
    ggplot(aes(pc, pve, group = 1)) +
    geom_line() +
    geom_point()
```

## 10.5 - Clustering

### 10.5.1 - K-means clustering

The `kmeans()` function is used to perform K-means clustering. Let's create a random data set where 25 observations have a mean shift.

```{r 8}
set.seed(1)
random_data <- tibble(x = rnorm(50), y = rnorm(50))

random_data <- random_data %>%
    mutate(x = x-3, y = y+6) %>%
    sample_n(25) %>%
    full_join(random_data, copy = T)

data_kmeans <- kmeans(random_data, 2, nstart = 20)

random_data %>%
    mutate(group = as.factor(data_kmeans$cluster)) %>%
    ggplot(aes(x,y)) +
    geom_point(aes(colour = group))
```

The cluster assignments are in the `$cluster` variable. We can see that they have been perfectly separated into their groups.

In real life we don't know how many clusters there are - let's try it with 3 groups.

```{r 9}
data_kmeans <- kmeans(random_data, 3, nstart = 20)

random_data %>%
    mutate(group = as.factor(data_kmeans$cluster)) %>%
    ggplot(aes(x,y)) +
    geom_point(aes(colour = group))
```

The `nstart` argument to `kmeans()` causes it to run with multiple random group assignments, and the function only reports the best results.

The `$tot.withinss` variable is the total within cluster sum of squares which we're seeking to minimise: 

$$ \text{minimise}_{C_1, \ldots, C_K} \bigg\{ \sum_{k=1}^K W(C_k) \bigg\} $$

### 10.5.2 - Hierarchical Clustering

The `hclust()` function implements hierarchical clustering. We begin by clustering our previous random data observations using complete linkage, and use the `dist()` function to compute the 50x50 inter-observation Euclidean distance matrix.

```{r 10}
h_clust_comp <- random_data %>%
    dist() %>%
    hclust(method = 'complete')

h_clust_av <- random_data %>%
    dist() %>%
    hclust(method = 'average')

h_clust_sngle <- random_data %>%
    dist() %>%
    hclust(method = 'single')
```

We use the `ggdendrogram()` function from the `ggdendro` library to render the dendrogram:

```{r 11}
ggdendrogram(h_clust_comp) + labs(title = "Complete Linkage")
ggdendrogram(h_clust_av) + labs(title = "Average Linkage")
ggdendrogram(h_clust_sngle) + labs(title = "Single Linkage")
```

To determine the the cluster labels you can use the `cutree()` function, which determines where on the dendrogam the cut should be made. The `k = ` argument determines the number of groups (like the K in k-means) or the `h = ` argument determines the heigh on the dendrogram. The first argument is `k`.

```{r 12}
h_clust_comp %>% cutree(2)
h_clust_comp %>% cutree(3)
```

We should scale the variables before performing the clustering:

```{r 13}
random_data %>%
    scale() %>%
    dist() %>%
    hclust() %>%
    ggdendrogram() +
    labs(title = 'Dendrogram with Scaling')
```


### 10.6 - NCI60 Data Example

Unsupervised techniques are often used in the analysis of genomic data. We take a look at the NCI60 cancer cell microarray data. It consists of gene expression measurements on cancer cells.

We perform PCA on the data after scaling the variables, then visualise the first couple of principal components.

```{r 14}
nci_data <- NCI60$data
nci_pca <- prcomp(nci_data)
autoplot(nci_pca)
autoplot(nci_pca, x = 2, y = 3)
```

We take a look at the proportion of variance explained and the cumulative variance explained via the summary and graph it:

```{r 15}
tibble(var_exp = summary(nci_pca)$importance[2,]) %>%
    mutate(n = row_number()) %>%
    ggplot(aes(n,var_exp)) +
    geom_point() +
    geom_line() +
    labs(title = "Variance Explained")

tibble(cum_var = summary(nci_pca)$importance[3,]) %>%
    mutate(n = row_number()) %>%
    ggplot(aes(n,cum_var)) +
    geom_point() +
    geom_line() +
    labs(title = 'Cumulative Variance Explained')
```



### 10.6.2 - Clustering the NCI60 Data

We now look at using hierarchical clustering on the NCI60 data. We first scale the observations to have a mean of 0 and a standard deviation of one.

We then perform clustering with complete linkage, using Euclidian distance as the similarity measure.

```{r 16}
nci_scaled <- scale(NCI60$data)
nci_hc <- nci_scaled %>% 
    dist() %>% 
    hclust()

nci_hc %>% ggdendrogram()
```

We then cut the dendrogram at a height that will yield four clusters.

```{r 17}
nci_hc %>% 
    cutree(4) -> nci_four_cluster

table( nci_four_cluster, NCI60$labs )
```

We can see some clear patterns, with `OVARIAN` and `LEUKEMIA` falling in the same category.

Lets compare the K-means clustering against the hierarchical clustering:

```{r 18}
nci_kmeans <- NCI60$data %>% kmeans(4, nstart = 20)
nci_kmeans_four_cluster <- nci_kmeans$cluster
table( nci_kmeans_four_cluster, nci_four_cluster )
```

We see diferences in the clustering allocations.

Rather than perform hierarchical clustering on the entire matrix, we can perform it on the first few principal component scores.

```{r 19}
NCI60$data %>%
    prcomp(scale = T) %>%
    .$x %>% .[,1:5] %>%
    dist() %>%
    hclust() -> nci_hc_pca

ggdendrogram(nci_hc_pca)
```

    Not surprisingly the results are different. Sometimes performing clustering on the first few principal components can give better results. We might view the PCA step as 'de-noising' the data.


