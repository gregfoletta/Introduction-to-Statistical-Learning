# Chapter 2 - Notes

## 2.1 - Statistical Learning 

**Statistical Learning** - a set of approaches for estimating a function f(X) from a set of data.



### 2.1.1 - Why estimate `f`? 

Either for prediction or inference.

**Prediction**
The set of inputs `X` are readily available, but the output `Y` cannot easily be obtained. We wish to predict what `Y` will be given a set of inputs `X`.

The accuracy of the prediction depends on two quantities - the *reducible* and the *irreducible* error. The estimate `f_hat(x)` will not be a perfect estimate for the true `f(x)`. We can reduce the error by using better statistical learning techniques.

However the variability of `e` also affects the accuracy of the predictions. This is the irreducible error.

**Inference**

We are often interested in the way that `Y` is affected by changes in `X`. We need to estimate `f(x)`, but we don't want to predict `Y`. We instead want to understand the relationship betrween `X` and `Y`.

### 2.1.2 - Estimating f(...) 

**Parametric methods** of estimating `f(...)` involve a two-step approach:

1. Make an assumption about the functional form (linear, quadratic, logarithmic, etc)
2. Fit or train the model.

This reduces the problem of estimating `f` down to estimating a set of parameters.

**Non-parametric methods** do not make explicit assumptions about the functional form. The disadvantage is that as they don't reduce the problem of estimating `f` to a small number of parameters, a very large number of observations is required to obtain an accurate estimate for `f`.

### 2.1.4 - Supervised vs Unsupervised Learning

**Supervised Learning** - For each observation of the predictor measurements x_i, i = 1,...,n there is an associated response measurement y_i. We fit a model that relates the response to the predictors.

**Unsupervised Learning** - For each observation i = 1,...,n we observce x_i, but no associated response y_i. A model cannot be fit since there is no response variable. Instead we seek to understand the relationsips between the variables.

### 2.1.5 - Regression vs Classification

Variables can be quantitative or qualitative (also known as categorical). We tend to refer to problems with a quantative response as *regression* problems, and those with a qualitative response as *classification* problems.

## 2.2 - Assessing Model Accuracy

There is no free lunch in statistics - one method may work on one data set, but another method may work better on a similar but different data set.

### 2.2.1 Measuring the Quality of Fit

To quantify the extent to which a model's predictions match the observed data, we use the *mean squared error*, or **MSE**. It is the sum of the square of the difference between the actual values and the observed values, divided by the number of observations.

The MSE is small if the predicted responses are very close to the true responses.

For example, imagine we have 5 actual responses to predictions (`y_i`), and 5 predictions given by our model (`f_hat_of_x`).
```{r 2.2.1_2}
set.seed(1)
y_i <- rnorm(5)
f_hat_of_x <- rnorm(5)
sum((y_i - f_hat_of_x)^2)/length(y_i)
```

We don't usually care about the *training MSE*, but moreso in the *test MSE*.

### 2.2.2 - Bias / Variance Trade-off

**Variance** refers to the amount by which `f_hat` would change if we estimated it using a different set of training data. In general, more flexible statistical methods have higher variance.

**Bias** refers to the error that is introduced by approximating a real life problem with a simpler model. More flexible methds result in less bias.

As a general rule, as we use more flexible methods, the variance will increase and the bias will decrease. The relative rate of change of these two quantities determines whether the test MSE increases or decreases.

### 2.2.3 - The Classification Setting

The **training error rate** is the most common approach for quantifying the accuracy of `f_hat` when `y_i` is no longer quantitative but qualitative. It is the sum of the *indicator variable* `I(y_i != f_hat_of_x)` divided by the number of observations. The indicator variable is 1 when `y_i != f_hat_of_x`, and 0 when it does. If the indicator variable is 0, our classifier correctly classified the response. 

We can show this in R:
```{r 2.2.3_2}
set.seed(1)
y_i <- c(rep('a', 100), rep('b', 100), rep('c', 100)) %>% sample(100)
f_hat_of_x <- c(rep('a', 100), rep('b', 100), rep('c', 100)) %>% sample(100)

sum(y_i != f_hat_of_x) / length(y_i)
```

We see that our training error rate in this instance is 0.66, or 66%. 

#### The Bayes Classifier

The **test error rate** is minimised, on average, by a simple classifier that *assigned each observation to the most likely class, given its predictor values**.







